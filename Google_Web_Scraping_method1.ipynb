{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d63a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import usaddress\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4cb113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to search keywords and scrape URL from google result page\n",
    "def google_search(university_list,keyword_list):\n",
    "    URL_list=pd.DataFrame(columns=('title',\n",
    "    'link',\n",
    "    'university',\n",
    "    'keyword',\n",
    "    'rank'))\n",
    "    for university in university_list:\n",
    "        for keyword in keyword_list:\n",
    "            search_word='+'.join((university+'+'+keyword).split())\n",
    "            URL = f\"https://google.com/search?q={search_word}\".format(search_word=search_word)\n",
    "            resp = requests.get(URL, headers=headers)\n",
    "            soup = BeautifulSoup(resp.content, \"html.parser\")\n",
    "            rank_num = 1\n",
    "            for g in soup.find_all('div', class_='r'):\n",
    "                anchors = g.find_all('a')\n",
    "                if anchors:\n",
    "                    link = anchors[0]['href']\n",
    "                    title = g.find('h3').text\n",
    "                    item = {\n",
    "                        \"title\": title,\n",
    "                        \"link\": link,\n",
    "                        'university':university,\n",
    "                        'keyword':keyword,\n",
    "                        'rank':rank_num\n",
    "                        }\n",
    "                    URL_list=URL_list.append(item,ignore_index=True)\n",
    "                rank_num=rank_num+1\n",
    "        URL_list.to_csv('URL_list.csv')\n",
    "    return URL_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3466bd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to parse address from webpage\n",
    "def url_postal_adddress(url):\n",
    "#     USER_AGENT = \"Mozilla/5.0 (Linux; <Android Version>; <Build Tag etc.>) AppleWebKit/<WebKit Rev>(KHTML, like Gecko) Chrome/<Chrome Rev> Safari/<WebKit Rev>\"\n",
    "    USER_AGENT = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36\"\n",
    "    headers = {\"user-agent\" : USER_AGENT}\n",
    "    \n",
    "    RE_address=re.compile(r\"\"\"(\n",
    "                \\d{1,5}[ ]    #street number\n",
    "                (?:E\\.[ ]|S\\.[ ]|W\\.[ ]|N\\.[ ])?    #east south west north prefix\n",
    "                [a-zA-Z ]{1,20}    #street name\n",
    "                (?:street|st|avenue|ave|road|rd|highway|hwy|square|sq|trail|trl|drive|dr|court|ct|park|parkway|pkwy|circle|cir|boulevard|blvd)\\W?(?=\\s)\\s*    #street surffix\n",
    "                (?:\\w{10})?\n",
    "                [a-zA-Z\\']{1,14}(?:\\s[a-zA-Z\\']{1,14}){0,1} #city\n",
    "                (?:,\\s|\\b)?(?:AL|AK|AR|AZ|CA|CO|CT|DC|DE|FL|GA|HI|IA|ID|IL|IN|KS|KY|LA|MA|MD|ME|MI|MN|MO|MS|MT|NC|ND|NE|NH|NJ|NM|NV|NY|OH|OK|OR|PA|RI|SC|SD|TN|TX|UT|VA|VT|WA|WI|WV|WY) #state\n",
    "                (?:,\\s|\\s)?\\b\\d{5}(?:[-\\s]\\d{4})?\\b\n",
    "                 )\"\"\", re.IGNORECASE | re.VERBOSE)\n",
    "    \n",
    "    address_parsed_list=pd.DataFrame(columns=('AddressRaw',\n",
    "    'AddressNumberPrefix',\n",
    "    'AddressNumber',\n",
    "    'AddressNumberSuffix',\n",
    "    'StreetNamePreModifier',\n",
    "    'StreetNamePreDirectional',\n",
    "    'StreetNamePreType',\n",
    "    'StreetName',\n",
    "    'StreetNamePostType',\n",
    "    'StreetNamePostDirectional',\n",
    "    'SubaddressType',\n",
    "    'SubaddressIdentifier',\n",
    "    'BuildingName',\n",
    "    'OccupancyType',\n",
    "    'OccupancyIdentifier',\n",
    "    'CornerOf',\n",
    "    'LandmarkName',\n",
    "    'PlaceName',\n",
    "    'StateName',\n",
    "    'ZipCode',\n",
    "    'USPSBoxType',\n",
    "    'USPSBoxID',\n",
    "    'USPSBoxGroupType',\n",
    "    'USPSBoxGroupID',\n",
    "    'IntersectionSeparator',\n",
    "    'Recipient'))\n",
    "    \n",
    "    resp = requests.get(url)\n",
    "    soup = BeautifulSoup(resp.content, \"html.parser\")\n",
    "    website_text=re.sub('\\s+',' ',soup.text)\n",
    "    address_list=re.findall(RE_address,website_text)\n",
    "    \n",
    "    for address in address_list:\n",
    "        address_parsed=np.flip(usaddress.parse(address),1).T\n",
    "        address_df = pd.DataFrame(data=address_parsed,columns=address_parsed[0,:])\n",
    "        address_df = address_df[1:]\n",
    "        address_df['AddressRaw']=address\n",
    "        address_df = address_df.groupby(address_df.columns, axis=1).apply(lambda x: x.apply(lambda y: ' '.join([l for l in y if l is not None]), axis=1))\n",
    "        address_parsed_list=address_parsed_list.append(address_df)\n",
    "    return address_parsed_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a99211",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load university names and define keywords\n",
    "university_list=pd.read_csv(\"universities.csv\",encoding='latin1')['University_Name']\n",
    "keyword_list=['dormitory address', 'residential hall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824ca27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#search and scrape URL from google result page\n",
    "url_list=google_search(university_list,keyword_list)\n",
    "# url_list.to_csv('URL_list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27a2615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each URL, scrape and parse the address and store in URL_address_all\n",
    "URL_address_all = pd.DataFrame()\n",
    "URL_list_missed = pd.DataFrame()\n",
    "for index, row in url_list.iterrows():\n",
    "    try:\n",
    "        row_df=pd.DataFrame(row).T\n",
    "        row_df['join_key']=1\n",
    "        address_df=url_postal_adddress(row['link'])\n",
    "        address_df['join_key']=1\n",
    "        URL_address=pd.merge(row_df,address_df).drop(columns=['Unnamed: 0','join_key'])\n",
    "        if URL_address.size>0:\n",
    "            if URL_address_all.size > 0:\n",
    "                URL_address_all=URL_address_all.append(URL_address)\n",
    "            else:\n",
    "                URL_address_all=URL_address\n",
    "        else:\n",
    "            if URL_list_missed.size==0:\n",
    "                URL_list_missed=row.to_frame().T\n",
    "            else:\n",
    "                URL_list_missed=URL_list_missed.append(row)\n",
    "#         URL_address_all.to_csv('university_address.csv')\n",
    "        print(URL_address.size)\n",
    "    except:\n",
    "        if URL_list_missed.size==0:\n",
    "            URL_list_missed=row.to_frame()\n",
    "        else:\n",
    "            URL_list_missed=URL_list_missed.append(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
